{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Этап L4"
      ],
      "metadata": {
        "id": "sKTf5a3IIgmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm"
      ],
      "metadata": {
        "id": "AOWnQyNNuhW0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lightgbm"
      ],
      "metadata": {
        "id": "5H9mgh-8ui1b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import save, load\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "import itertools\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import recall_at_k, precision_at_k, auc_score\n",
        "import lightgbm"
      ],
      "metadata": {
        "id": "1bgBAvMSul6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/WB School/data.csv.gzip\"\n",
        "df = pd.read_csv(path, compression=\"gzip\")\n",
        "df[\"order_ts\"] = pd.to_datetime(df[\"order_ts\"])"
      ],
      "metadata": {
        "id": "bSVZePeSuttU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание классов"
      ],
      "metadata": {
        "id": "5H6Q7rlDIczc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataPreparation"
      ],
      "metadata": {
        "id": "lnl6vZgfi_6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс **DataPreparation** позволяет выполнить все необходимые преобразования над данными для дальнейшего обучения моделей.\n",
        "\n",
        "Метод extract_reluctant_users исключит из рассмотрения тех пользователей, которые взаимодействовали с товарами слишком малое число раз. Метод drop_rare_items служит для аналогичной цели с той разницей, что отбрасывает слишком редко заказываемые товары.\n",
        "\n",
        "Метод train_test поделит выборку на обучающую и тестовую части либо по времени, либо по пропорции между выборками.\n",
        "\n",
        "Метод common_only оставит только те взаимодействия, которые: (1) осуществлены пользователями, сделавшими заказ и в обучающей, и в тестовой частяъ выборки; (2) связаны с товарами, заказанными и в обучающей, и в тестовой выборках. С рациональной точки зрения, оценить качество модели по пользователю, принадлежащему лишь одной выборке, не получится, поэтому их можно отбросить. С технической точки зрения, это снизит вычислительную сложность.\n",
        "\n",
        "Метод csr_matrix_via_encoder построит из обучающей и тестовой выборок 2 разрезженные матрицы csr-формата, причём пользователи и товары получат одинаковую индексацию в обеих новых матрицах."
      ],
      "metadata": {
        "id": "Famcx_51zryL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreparation:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def extract_reluctant_users(self, data, threshold=5, both=False):\n",
        "    \"\"\"\n",
        "    Метод отбрасывает/отделяет пользователей, которые сделали заказы в количестве, меньшем порогового значения.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "      pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    threshold: int\n",
        "      пороговое целочисленное значение минимального количества заказов у пользователя, достаточное для того, чтобы не быть отброшенным/отделённым.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "      pandas-датафрейм, все пользователи которого сделали по меньшей мере заказов в количестве, не меньшем порогового значения.\n",
        "    data_reluctants: Dataframe\n",
        "      Если указано both=True, то недостаточно активные пользователи будут не отброшены, а сохранены в отдельный pandas-датафрейм.\n",
        "    \"\"\"\n",
        "    data = data.drop_duplicates()\n",
        "    data_count = data.groupby([\"user_id\", \"item_id\"], as_index=False).count().rename(columns={\"order_ts\": \"counter\"})\n",
        "    data_count_users = data_count.groupby(\"user_id\", as_index=False)[\"counter\"].sum()\n",
        "    users = data_count_users.loc[data_count_users.counter <= threshold, \"user_id\"].values\n",
        "    data = data[~data.user_id.isin(users)]\n",
        "    if both == True:\n",
        "      data_reluctants = data[data.user_id.isin(users)]\n",
        "      return data_reluctants, data\n",
        "    else:\n",
        "      return data\n",
        "\n",
        "\n",
        "  def drop_rare_items(self, data, threshold=2):\n",
        "    \"\"\"\n",
        "    Метод отбрасывает товары, с которыми пользователи взаимодействовали меньше, чем threshold раз.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "      pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    threshold: int\n",
        "      пороговое целочисленное значение минимального количества заказов товара, достаточное для того, чтобы товар ге был отброшен.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "       pandas-датафрейм, в котором остались взаимодействия исключительно с достаточно популярными товарами, т.е. с товарами, которые были заказаны более, чем threshold раз.\n",
        "    \"\"\"\n",
        "    data_temp = data.drop_duplicates()\n",
        "    data_count = data_temp.groupby([\"user_id\", \"item_id\"], as_index=False).count().rename(columns={\"order_ts\": \"counter\"})\n",
        "    data_count_items = data_count.groupby(\"item_id\", as_index=False)[\"counter\"].sum()\n",
        "    items = data_count_items.loc[data_count_items.counter <= threshold, \"item_id\"].values\n",
        "    data = data[~data.item_id.isin(items)]\n",
        "    return data\n",
        "\n",
        "\n",
        "  def train_test(self, data, by=\"time\", test_weeks=1, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Метод делит выборку на обучающую и тестовую одним из двух способов: по времени, что необходимо для получения глобальных обучающей и тестовой выборок, и по доле, что используется для разделения на локальные обучающую и тестовую выборки.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "      pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    by: str\n",
        "      Строка, принимающая 2 значения. Если необходимо разделить по времени, то by='time'. Если же по долям, то by='percents'.\n",
        "    test_weeks: float\n",
        "      Действительное ненулевое число, обозначающее, сколько недель необходимо отнести в тестовую выборку, если указано by='time' и разбиение необходимо сделать по времени.\n",
        "    test_size: float\n",
        "      Действительное число, отражающее проворцию между размером тестовой выборки и размером исходной выборки, если указано by='percents'.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    train: Dataframe\n",
        "      Обучающая выборка.\n",
        "    test: Dataframe\n",
        "      Тестовая выборка.\n",
        "    \"\"\"\n",
        "    if by == \"time\":\n",
        "      n_folds = 13 / test_weeks\n",
        "      delta = (data[\"order_ts\"].max() - data[\"order_ts\"].min()) / n_folds\n",
        "      edge = data[\"order_ts\"].max() - delta\n",
        "      train = data.loc[data[\"order_ts\"] <= edge]\n",
        "      test = data.loc[data[\"order_ts\"] > edge]\n",
        "    elif by == \"percents\":\n",
        "      train_size = 1 - test_size\n",
        "      idx = int(len(data) * train_size)\n",
        "      train = data[:idx]\n",
        "      test = data[idx:]\n",
        "    return train, test\n",
        "\n",
        "\n",
        "  def common_only(self, data_first, data_second, both=True, common_col=\"user_id\"):\n",
        "    \"\"\"\n",
        "    Метод оставляет в обоих датафреймах только те наблюдения, которые принадлежат пересечению по колонке column.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data_first: Dataframe\n",
        "      Первый pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    data_second: Dataframe\n",
        "      Второй pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    column: str\n",
        "      Название колонки, по которой надо исхать общие значения. Принимает 2 значения: 'user_id' или 'item_id'.\n",
        "\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    data_first: Dataframe\n",
        "      Первый датафрейм, в котором остались только те наблюдения, которые имеют общие значения по колонке column со вторым датафреймом data_second.\n",
        "    data_second: Dataframe\n",
        "      Первый датафрейм, в котором остались только те наблюдения, которые имеют общие значения по колонке column с первым датафреймом data_first.\n",
        "    \"\"\"\n",
        "    if both == True:\n",
        "\n",
        "      column = \"user_id\"\n",
        "      common = list(set(data_first[column]).intersection(set(data_second[column])))\n",
        "      data_first = data_first[data_first[column].isin(common)]\n",
        "      data_second = data_second[data_second[column].isin(common)]\n",
        "\n",
        "      column = \"item_id\"\n",
        "      common = list(set(data_first[column]).intersection(set(data_second[column])))\n",
        "      data_first = data_first[data_first[column].isin(common)]\n",
        "      data_second = data_second[data_second[column].isin(common)]\n",
        "\n",
        "      column = \"user_id\"\n",
        "      common = list(set(data_first[column]).intersection(set(data_second[column])))\n",
        "      data_first = data_first[data_first[column].isin(common)]\n",
        "      data_second = data_second[data_second[column].isin(common)]\n",
        "\n",
        "      column = \"item_id\"\n",
        "      common = list(set(data_first[column]).intersection(set(data_second[column])))\n",
        "      data_first = data_first[data_first[column].isin(common)]\n",
        "      data_second = data_second[data_second[column].isin(common)]\n",
        "\n",
        "    elif both == False:\n",
        "\n",
        "      common = list(set(data_first[common_col]).intersection(set(data_second[common_col])))\n",
        "      data_first = data_first[data_first[common_col].isin(common)]\n",
        "      data_second = data_second[data_second[common_col].isin(common)]\n",
        "\n",
        "    return data_first, data_second\n",
        "\n",
        "\n",
        "  def csr_matrix_via_encoder(self, data_first, data_second):\n",
        "    \"\"\"\n",
        "    Метод преобразует два pandas-датафрейма в 2 разрезженные csr-матрицы с одинаковым порядком индексов пользователей и айтемов.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data_first: Dataframe\n",
        "      Первый pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "    data_second: Dataframe\n",
        "      Второй pandas-датафрейм с историей взаимодействий пользователей с товарами формата исходного датафрейма.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    data_first_sparse: csr-matrix\n",
        "      Разрезженная csr-матрица, соответствующая первому датафрейму data_first.\n",
        "    data_second_sparse: csr-matrix\n",
        "      Разрезженная csr-матрица, соответствующая второму датафрейму data_second.\n",
        "    data_first: Dataframe\n",
        "      Датафрейм, отличающийся от датафрейма data_first, поступившего на вход, наличием 2 колонок с новой индексацией для user_id и item_id, соответствующей отображению в csr-матрицу.\n",
        "    data_second: Dataframe\n",
        "      Датафрейм, отличающийся от датафрейма data_first, поступившего на вход, наличием 2 колонок с новой индексацией для user_id и item_id, соответствующей отображению в csr-матрицу.\n",
        "    \"\"\"\n",
        "    data_first = data_first.groupby([\"user_id\", \"item_id\"], as_index=False).count().rename(columns={\"order_ts\": \"counter\"})\n",
        "    data_second = data_second.groupby([\"user_id\", \"item_id\"], as_index=False).count().rename(columns={\"order_ts\": \"counter\"})\n",
        "\n",
        "    data_first = data_first.sort_values(\"user_id\")\n",
        "    data_second = data_second.sort_values(\"user_id\")\n",
        "\n",
        "    user_encoder, item_encoder = LabelEncoder(), LabelEncoder()\n",
        "\n",
        "    users_final = set(data_first.user_id.unique()).intersection(set(data_second.user_id.unique()))\n",
        "    user_encoder.fit(list(users_final))\n",
        "\n",
        "    all_items = set(data_first.item_id.unique()).union(set(data_second.item_id.unique()))\n",
        "    item_encoder.fit(list(all_items))\n",
        "\n",
        "    data_first[\"user_new_id\"] = user_encoder.transform(data_first[\"user_id\"])\n",
        "    data_second[\"user_new_id\"] = user_encoder.transform(data_second[\"user_id\"])\n",
        "\n",
        "    data_first[\"item_new_id\"] = item_encoder.transform(data_first[\"item_id\"])\n",
        "    data_second[\"item_new_id\"] = item_encoder.transform(data_second[\"item_id\"])\n",
        "\n",
        "    matrix_shape = len(user_encoder.classes_), len(item_encoder.classes_)\n",
        "\n",
        "    data_first_sparse = coo_matrix((list(data_first.counter.astype(np.float32)),\n",
        "                                   (list(data_first.user_new_id.astype(np.int64)),\n",
        "                                    list(data_first.item_new_id.astype(np.int64)))), shape=matrix_shape)\n",
        "    data_first_sparse = data_first_sparse.tocsr()\n",
        "\n",
        "    data_second_sparse = coo_matrix((list(data_second.counter.astype(np.float32)),\n",
        "                                    (list(data_second.user_new_id.astype(np.int64)),\n",
        "                                     list(data_second.item_new_id.astype(np.int64)))), shape=matrix_shape)\n",
        "    data_second_sparse = data_second_sparse.tocsr()\n",
        "\n",
        "    users = sorted(list(set(coo_matrix(data_first_sparse).row)))\n",
        "    items = sorted(list(set(coo_matrix(data_first_sparse).col)))\n",
        "\n",
        "    return data_first_sparse, data_second_sparse, data_first, data_second, users, items\n",
        "\n",
        "\n",
        "  def users_items(self, data_sparse):\n",
        "    \"\"\"\n",
        "    Метод извлекает user_id и item_id, для которых понадобится вычислять скоры.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data_sparse: csr_matrix\n",
        "      Разрезженная csr-матрица. В контексте задачи скоры необходимо считать по обучающей выборке, поэтому это матрица, соответствующая ей.\n",
        "    \"\"\"\n",
        "    users = sorted(list(set(coo_matrix(data_sparse).row)))\n",
        "    items = sorted(list(set(coo_matrix(data_sparse).col)))\n",
        "    return users, items"
      ],
      "metadata": {
        "id": "sDdbVDFOzqjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CandidatesExtractor"
      ],
      "metadata": {
        "id": "91BUuHYpjLpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс **CandidatesExtractor** отвечает за отбор кандидатов в ранжирование. Помимо идентификаторов товаро-кандидатов, на выход получим соответствующие им скоры и ранги, которые и будут фичами для градиентного бустинга, использующегося для ранжирования.\n",
        "\n",
        "Метод fit отвечает за обучение моделей из библиотеки [LightFM](https://making.lyst.com/lightfm/docs/home.html):\n",
        "\n",
        "*   Weighted Approximate-Rank Pairwise loss или [WARP](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf);\n",
        "*   Bayesian Personalized Ranking или [BPR](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf);\n",
        "*   Logistic Matrix Factorization или [LMF](https://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf);\n",
        "*   k-Order Statistic Weighted Approximate-Rank Pairwise loss или [k-OS WARP](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41534.pdf).\n",
        "\n",
        "Опробованные модели из библиотеки [implicit](https://github.com/benfred/implicit) показали себя хуже. Качество моделей по метрике Recall@20 среди моделей LightFM разнится несильно, поэтому в методе можно обучить все 4 модели. При этом из-за технических ограничений в RAM для отбора кандидатов следует использовать лишь 2 из них.\n",
        "\n",
        "Метод evaluate используется для оценки модели по 3 метрикам качества: ROC-AUC, Precision@K и Recall@K.\n",
        "\n",
        "В библиотеке LightFM не предусмотрено структурированное извлечение скоров айтемов. Это позволяет сделать метод calculate_scores. При том, что он работает довольно долго, RAM будет загружаться незначительно.\n",
        "\n",
        "Метод candidates_extraction отбирает кандидаты. Для воспроизведения результатов на использованных ранее данных можно не запускать вычисления заново, а загрузить и работать с предпосчитанными скорами товаров."
      ],
      "metadata": {
        "id": "NAR1Xmewmghh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Denq2OH10PZ"
      },
      "outputs": [],
      "source": [
        "class CandidatesExtractor:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def fit(self, train_data_csr, loss, parameters, random_state=42):\n",
        "    \"\"\"\n",
        "    Метод обучит модели первого уровня с оптимальными значениями параметров, подобранными заранее на основании кросс-валидации.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    train_data_csr: csr-matrix\n",
        "      Разрезженная csr-матрица, полученная из обучающей выборки.\n",
        "    loss: str\n",
        "      Функция потерь, отражающая, какого типа модель необходимо обучить. Принимает значения 'warp', 'bpr', 'logistic' и 'warp-kos'.\n",
        "    parameters: dict\n",
        "      Набор параметров для обучения модели.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    model: lightfm.LightFM\n",
        "      Обученная LightFM-модель с параметрами parameters.\n",
        "    \"\"\"\n",
        "    if loss == \"warp\":\n",
        "      model = LightFM(no_components=parameters[\"no_components\"],\n",
        "                      learning_schedule=parameters[\"learning_schedule\"],\n",
        "                      loss=loss,\n",
        "                      learning_rate=parameters[\"learning_rate\"],\n",
        "                      item_alpha=parameters[\"item_alpha\"],\n",
        "                      user_alpha=parameters[\"user_alpha\"],\n",
        "                      max_sampled=parameters[\"max_sampled\"],\n",
        "                      random_state=random_state)\n",
        "    elif loss == \"bpr\":\n",
        "      model = LightFM(no_components=parameters[\"no_components\"],\n",
        "                      learning_schedule=parameters[\"learning_schedule\"],\n",
        "                      loss=loss,\n",
        "                      learning_rate=parameters[\"learning_rate\"],\n",
        "                      item_alpha=parameters[\"item_alpha\"],\n",
        "                      user_alpha=parameters[\"user_alpha\"],\n",
        "                      random_state=random_state)\n",
        "    elif loss == \"logistic\":\n",
        "      model = LightFM(no_components=parameters[\"no_components\"],\n",
        "                      learning_schedule=parameters[\"learning_schedule\"],\n",
        "                      loss=loss,\n",
        "                      learning_rate=parameters[\"learning_rate\"],\n",
        "                      item_alpha=parameters[\"item_alpha\"],\n",
        "                      user_alpha=parameters[\"user_alpha\"],\n",
        "                      random_state=random_state)\n",
        "    elif loss == \"warp-kos\":\n",
        "      model = LightFM(no_components=parameters[\"no_components\"],\n",
        "                      k=parameters[\"k\"],\n",
        "                      n=parameters[\"n\"],\n",
        "                      learning_schedule=parameters[\"learning_schedule\"],\n",
        "                      loss=loss,\n",
        "                      learning_rate=parameters[\"learning_rate\"],\n",
        "                      item_alpha=parameters[\"item_alpha\"],\n",
        "                      user_alpha=parameters[\"user_alpha\"],\n",
        "                      max_sampled=parameters[\"max_sampled\"],\n",
        "                      random_state=random_state)\n",
        "    else:\n",
        "      print(\"В библиотеке LightFM такой модели нет. Параметр loss может принимать 1 из 4 значений: 'warp', 'bpr', 'logistic' или 'warp-kos'.\")\n",
        "\n",
        "    model.fit(train_data_csr, epochs=parameters[\"epochs\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "  def evaluate(self, model, test_data_csr, metric=\"recall\", K_items=20):\n",
        "    \"\"\"\n",
        "    Метод оценит качество модели по указанной метрике.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    model: lightfm.LightFM\n",
        "      Обученная модель из библиотеки LightFM.\n",
        "    test_data_csr: csr-matrix\n",
        "      Разрезженная csr-матрица, полученная из тестовой выборки.\n",
        "    metric: str\n",
        "      Метрика качества модели, по которой будет оцениваться модель. В библиотеке LightFM доступно 4 метрики качества: 'auc_score', 'precision_at_k', 'recall_at_k' и 'reciprocal_rank', однако последний принимать во внимание не будем.\n",
        "    K_items: int\n",
        "      Сколько айтемов учитывать при вычислении метрики качества. По умолчанию равно 20.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    metric_score: float\n",
        "      Значение указанной метрики качества.\n",
        "    \"\"\"\n",
        "    if metric == \"recall\":\n",
        "      metric_score = recall_at_k(model=model,\n",
        "                                 test_interactions=test_data_csr,\n",
        "                                 k=K_items).mean()\n",
        "    elif metric == \"precision\":\n",
        "      metric_score = precisiob_at_k(model=model,\n",
        "                                    test_interactions=test_data_csr,\n",
        "                                    k=K_items).mean()\n",
        "    elif metric == \"auc_score\":\n",
        "      metric_score = auc_score(model=model,\n",
        "                               test_interactions=test_data_csr,\n",
        "                               k=K_items).mean()\n",
        "    else:\n",
        "      print(\"Такой метрики нет или она (пока) не учтена. Параметр metric может принимать 1 из 3 значений: 'auc_score', 'precision_at_k', 'recall_at_k', причём для модели отбора кандидатов самым важным является 'recall_at_k'.\")\n",
        "\n",
        "    return metric_score\n",
        "\n",
        "\n",
        "  def calculate_scores(self, model, users, items, items_number=50):\n",
        "    \"\"\"\n",
        "    Вычисление скоров айтемов сразу для всех пользователей является технически тяжёлым и требует много RAM при большом числе юзеров и/или айтемов. Метод calculate_scores() вычислит скоры без сильной нагрузки на RAM, но\n",
        "    для этого потребуется время. Функция применяется, когда необходимо посчитать скоры, т.к. нет предрассчитанных скоров.\n",
        "\n",
        "    Параметры:\n",
        "    model: lightfm.LightFM\n",
        "      Предварительно обученная модель из библиотеки LightFM.\n",
        "    users: list\n",
        "      Пользователи, для которых будем считать скоры айтемов и отбирать кандидатов.\n",
        "    items: list\n",
        "      Товары, среди которых будем отбирать товары-кандидаты в ранжирование для второго этапа.\n",
        "    items_number: int\n",
        "      Количество товаров-кандидатов от модели model. Если взять значение больше 50, то вычисления могут не поместиться в оперативную память, поэтому следует брать 50 айтемов.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    pairs: list\n",
        "      Множество пар вида (айтем, скор) для каждого пользователя из users.\n",
        "    \"\"\"\n",
        "    user_biases = model.user_biases[users]\n",
        "    item_biases = model.item_biases[items]\n",
        "    item_embeddings = model.item_embeddings[items]\n",
        "    user_embeddings = model.user_embeddings[users]\n",
        "\n",
        "    first_N_scores = user_embeddings.dot(item_embeddings[:items_number].T) + user_biases.reshape(-1,1) + item_biases[:items_number].reshape(1,-1)\n",
        "\n",
        "    pairs = list()\n",
        "\n",
        "    # Нумеруем первые N айтемов, чтобы не потеряться в нумерации, ведь она не совпадает с исходными item_id\n",
        "    for i in range(len(first_N_scores)):\n",
        "      user_scores = list()\n",
        "      for elem in enumerate(first_N_scores[i]):\n",
        "        user_scores.append(elem)\n",
        "      pairs.append(user_scores)\n",
        "\n",
        "    # Отбираем N (=items_number) айтемов с наибольшим скором, которые и будут кандидатами от модели model\n",
        "    for u in tqdm(range(len(user_embeddings))):\n",
        "      for i in range(items_number, len(item_embeddings)):\n",
        "        score = list(user_embeddings[u:(u + 1)].dot(item_embeddings[i:(i+1)].T) + user_biases[:1].reshape(-1,1) + item_biases[i:(i+1)].reshape(1,-1))[0][0]\n",
        "        pair = (i, score)\n",
        "        pairs[u].append(pair)\n",
        "        pairs[u] = sorted(pairs[u], key=lambda x: x[-1], reverse=True)\n",
        "        pairs[u].remove(pairs[u][-1])\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "  def candidates_extraction(self, model_type, path, prefix, users, items, model=None, top=50, items_number=50, precomputed_scores=True):\n",
        "    \"\"\"\n",
        "    Метод отбирает кандидатов от модели, которые пойдут на вход в итоговое ранжирование.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    model: lightfm.LightFM\n",
        "      Предварительно обученная модель из библиотеки LightFM.\n",
        "    model_type: str\n",
        "      Название модели, их которой извлекаются кандидаты. Необходимо для загрузки предпосчитанных скоров. Принимает значения 'warp', 'bpr', 'lmf' и 'warp_kos'.\n",
        "    path: str\n",
        "      Путь к скорам товаров-кандидатов от модели для каждого юзера из users. Необходимо для загрузки предпосчитанных скоров.\n",
        "    prefix: str\n",
        "      Для разных выборок из юзеров могут быть разные названия файлов со скорами, поэтому параметр позволит хранить все эмбеддинги.\n",
        "    users: list\n",
        "      Пользователи, для которых будем считать скоры айтемов и отбирать кандидатов. Потребуется, если скоры предварительно не вычислены.\n",
        "    items: list\n",
        "      Товары, среди которых будем отбирать товары-кандидаты в ранжирование для второго этапа. Потребуется, если скоры предварительно не вычислены.\n",
        "    top: int\n",
        "      Сколько кандидатов от модели необходимо извлечь. Болшое значение может привести к тому, что вычисления не поместятся в RAM.\n",
        "    items_number: int\n",
        "      Количество товаров-кандидатов от модели model. Если взять значение больше 50, то вычисления могут не поместиться в оперативную память, поэтому следует брать 50 айтемов. Тоже потребуется только для вычисления скоров.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    model_pairs: numpy ndarray\n",
        "      Множество пар вида (товар, скор) для каждого пользователя из users.\n",
        "    model_dict: dict\n",
        "      Словарь, ключами которого явлюятся id пользователей, соответствующими им значениями - id товаров-кандидатов от модели.\n",
        "    \"\"\"\n",
        "    if precomputed_scores == True:\n",
        "      path_pairs = path + model_type + \"_pairs_\" + prefix + \".npy\"\n",
        "      pairs = load(path_pairs)\n",
        "    elif precomputed_scores == False:\n",
        "\n",
        "      user_biases = model.user_biases[users],\n",
        "      item_biases = model.item_biases[items]\n",
        "      item_embeddings = model.item_embeddings[items]\n",
        "      user_embeddings = model.user_embeddings[users]\n",
        "\n",
        "      first_N_scores = user_embeddings.dot(item_embeddings[:items_number].T) + user_biases.reshape(-1,1) + item_biases[:items_number].reshape(1,-1)\n",
        "\n",
        "      pairs = list()\n",
        "\n",
        "      # Нумеруем первые N айтемов, чтобы не потеряться в нумерации, ведь она не совпадает с исходными item_id\n",
        "      for i in range(len(first_N_scores)):\n",
        "        user_scores = list()\n",
        "        for elem in enumerate(first_N_scores[i]):\n",
        "          user_scores.append(elem)\n",
        "        pairs.append(user_scores)\n",
        "\n",
        "      # Отбираем N (=items_number) айтемов с наибольшим скором, которые и будут кандидатами от модели model\n",
        "      for u in tqdm(range(len(user_embeddings))):\n",
        "        for i in range(items_number, len(item_embeddings)):\n",
        "          score = list(user_embeddings[u:(u + 1)].dot(item_embeddings[i:(i+1)].T) + user_biases[:1].reshape(-1,1) + item_biases[i:(i+1)].reshape(1,-1))[0][0]\n",
        "          pair = (i, score)\n",
        "          pairs[u].append(pair)\n",
        "          pairs[u] = sorted(pairs[u], key=lambda x: x[-1], reverse=True)\n",
        "          pairs[u].remove(pairs[u][-1])\n",
        "\n",
        "    model_dict = dict()\n",
        "    for user, user_data in enumerate(pairs):\n",
        "          for rank, (item, score) in enumerate(user_data):\n",
        "              key = tuple([user, item])\n",
        "              value = tuple([score, (rank + 1)])\n",
        "              model_dict[key] = value\n",
        "\n",
        "    model_pairs = list()\n",
        "    for key in model_dict.keys():\n",
        "        model_pairs.append(key)\n",
        "\n",
        "    return model_pairs, model_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HybridRecommender"
      ],
      "metadata": {
        "id": "aOhh1FuBjPY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс **HybridRecommender** реализует модель 2 этапа, ответственную за ранжирование товаров-кандидатов.\n",
        "\n",
        "Метод prepare_data преобразовывает данные к требуемому формату, который ожидает на вход бустинг.\n",
        "\n",
        "Метод train_test разделяет выборку на обучающую и тестовую части.\n",
        "\n",
        "Метод fit обучает модель градиентного бустинга.\n",
        "\n",
        "Метод recommend составляет рекомендации на основе обученной модели бустинга."
      ],
      "metadata": {
        "id": "Fav2GrJm3KEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridRecommender:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def prepare_data(self, model_first_pairs, model_second_pairs, model_first_dict, model_second_dict, model_first_name, model_second_name, test_global_csr, train, test):\n",
        "    \"\"\"\n",
        "    Метод составляет датафрейм для последующей подачи на вход бустингу, который выполнит итоговое ранжирование.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    model_first_pairs: list\n",
        "      Множество пар вида (товар, скор) для каждого пользователя из users от первой модели. Этот и последующие 3 параметра реализуются методом candidates_extractor.candidates_extraction().\n",
        "    model_second_pairs: list\n",
        "      Множество пар вида (товар, скор) для каждого пользователя из users от второй модели.\n",
        "    model_first_dict: dict\n",
        "      Словарь, ключами которого явлюятся id пользователей, соответствующими им значениями - id товаров-кандидатов от первой модели.\n",
        "    model_second_dict: dict\n",
        "      Словарь, ключами которого явлюятся id пользователей, соответствующими им значениями - id товаров-кандидатов от второй модели.\n",
        "    model_first_name: str\n",
        "      Название первой модели. Используется для присвоения имён колонок итогового датафрейма dataset. Для того же служит и следующий параметр.\n",
        "    model_second_name: str\n",
        "      Название второй модели.\n",
        "    test_global_csr: csr-matrix\n",
        "      Разрезженная csr-матрица, построенная по тестовому датафрейму.\n",
        "    train: Dataframe\n",
        "      pandas-датафрейм, использовавшийся для обучения моделей первого уровня в виде csr-матрицы.\n",
        "    test: Dataframe\n",
        "      pandas-датафрейм, использовавшийся для тестирования моделей первого уровня в виде csr-матрицы.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    dataset: Dataframe\n",
        "      pandas-датафрейм с исходными индексами пользователей и товаров, который будет использоваться для обучения и тестирования бустинга.\n",
        "    \"\"\"\n",
        "    total_pairs = list(set(model_first_pairs).union(set(model_second_pairs)))\n",
        "    del model_first_pairs, model_second_pairs\n",
        "\n",
        "    data_all_pairs = [pair +\n",
        "                      model_first_dict.get(pair, (np.nan, np.nan)) +\n",
        "                      model_second_dict.get(pair, (np.nan, np.nan))  for pair in tqdm(total_pairs)]\n",
        "    del model_first_dict, model_second_dict\n",
        "\n",
        "    first_score = model_first_name + \"_score\"\n",
        "    second_score = model_second_name + \"_score\"\n",
        "    first_rank = model_first_name + \"_rank\"\n",
        "    second_rank = model_second_name + \"_rank\"\n",
        "    data_all_pairs_df = pd.DataFrame(data_all_pairs,\n",
        "                                 columns=[\"user_id\", \"item_id\", first_score, first_rank,\n",
        "                                                                second_score, second_rank])\n",
        "    del data_all_pairs\n",
        "\n",
        "    for column in data_all_pairs_df.columns:\n",
        "      if column.endswith(\"id\"):\n",
        "        data_all_pairs_df[column] = data_all_pairs_df[column].astype(np.int32)\n",
        "      else:\n",
        "        data_all_pairs_df[column] = data_all_pairs_df[column].astype(np.float32)\n",
        "\n",
        "    purchases = list()\n",
        "    for k in range(test_global_csr.shape[0]):\n",
        "      cx = coo_matrix(test_global_csr[k])\n",
        "      purchased_items, user_id = [], []\n",
        "      user_id.append(k)\n",
        "      for i,j,v in zip(cx.row, cx.col, cx.data):\n",
        "        purchased_items.append(j)\n",
        "      for i in list(itertools.product(user_id, purchased_items)):\n",
        "        purchases.append(i)\n",
        "    del test_global_csr\n",
        "\n",
        "    data_true = {}\n",
        "    for i in purchases:\n",
        "      curr, item = i[0], int(i[1])\n",
        "      if curr not in data_true:\n",
        "        data_true[curr] = list()\n",
        "        data_true[curr].append(item)\n",
        "      else:\n",
        "        data_true[curr].append(item)\n",
        "    for i in tqdm(data_true.keys()):\n",
        "      data_true[i] = set(data_true[i])\n",
        "    del purchases\n",
        "\n",
        "    items_dict = dict(zip(train.item_new_id, train.item_id))\n",
        "    users_dict = dict(zip(train.user_new_id, train.user_id))\n",
        "    del train\n",
        "\n",
        "    data_all_pairs_df[\"user_id\"] = data_all_pairs_df[\"user_id\"].map(users_dict)\n",
        "    data_all_pairs_df[\"item_id\"] = data_all_pairs_df[\"item_id\"].map(items_dict)\n",
        "    del items_dict, users_dict\n",
        "\n",
        "    test[\"target\"] = 1\n",
        "    dataset = pd.merge(data_all_pairs_df,\n",
        "                       test[[\"user_id\", \"item_id\", \"target\"]].drop_duplicates(),\n",
        "                       how=\"left\",\n",
        "                       left_on=[\"user_id\", \"item_id\"],\n",
        "                       right_on=[\"user_id\", \"item_id\"])\n",
        "\n",
        "    dataset[\"target\"].fillna(0, inplace=True)\n",
        "    del test\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def train_test(self, data, train_size, random_state=42):\n",
        "    \"\"\"\n",
        "    Метод делит выборку, подготовленную для бустинга, на обучающую и тестовую части.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    data: Dataframe\n",
        "      pandas-датафрейм с фичами (ранги и скоры), полученными из моделей отбора кандидатов, и бинаризованным таргетом, обозначающим, произошло ли взаимодействие пользователя с товаром.\n",
        "    train_size: float\n",
        "      Размер обучающей выборки, на которой будет обучаться градиентный бустинг.\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    X_train: Dataframe\n",
        "      pandas-датафрейм из фичей для обучения градиентного бустинга.\n",
        "    y_train: Series\n",
        "      pandas-ряд, содержащий значения бинаризованного таргета, для обучения градиентного бустинга.\n",
        "    X_test: Dataframe\n",
        "      pandas-датафрейм из фичей для тестирования градиентного бустинга.\n",
        "    y_test: Series\n",
        "      pandas-ряд, содержащий значения бинаризованного таргета, для тестирования градиентного бустинга.\n",
        "    train_query: Series\n",
        "      Правило для разбиение обучающей выборки по пользователям, необходимый для градиентного бустинга.\n",
        "    test_query: Series\n",
        "      Правило для разбиение тестовой выборки по пользователям, необходимый для градиентного бустинга.\n",
        "    \"\"\"\n",
        "\n",
        "    train_xy, test_xy = train_test_split(data, train_size=0.7, random_state=42)\n",
        "\n",
        "    y_train = train_xy.pop(\"target\")\n",
        "    x_train = train_xy.copy()\n",
        "\n",
        "    y_test = test_xy.pop(\"target\")\n",
        "    x_test = test_xy.copy()\n",
        "\n",
        "    X_train = x_train[[\"warp_score\", \"warp_rank\", \"lmf_score\", \"lmf_rank\"]]\n",
        "    X_test = x_test[[\"warp_score\", \"warp_rank\", \"lmf_score\", \"lmf_rank\"]]\n",
        "\n",
        "    x_train = x_train.sort_values(\"user_id\").reset_index(drop=True)\n",
        "    x_test = x_test.sort_values(\"user_id\").reset_index(drop=True)\n",
        "\n",
        "    train_query = x_train[\"user_id\"].value_counts().sort_index()\n",
        "    test_query = x_test[\"user_id\"].value_counts().sort_index()\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, train_query, test_query\n",
        "\n",
        "\n",
        "  def fit(self, parameters, X_train, y_train, X_test, y_test, train_query, test_query, K_items=[20]):\n",
        "    \"\"\"\n",
        "    Метод обучает градиентный бустинг на полученных ранее фичах от моделей первого уровня.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    parameters: dict\n",
        "      Набор значений параметров для градиентного бустинга.\n",
        "    X_train: Dataframe\n",
        "      pandas-датафрейм из фичей для обучения градиентного бустинга.\n",
        "    y_train: Series\n",
        "      pandas-ряд, содержащий значения бинаризованного таргета, для обучения градиентного бустинга.\n",
        "    X_test: Dataframe\n",
        "      pandas-датафрейм из фичей для тестирования градиентного бустинга.\n",
        "    y_test: Series\n",
        "      pandas-ряд, содержащий значения бинаризованного таргета, для тестирования градиентного бустинга.\n",
        "    train_query: Series\n",
        "      Правило для разбиение обучающей выборки по пользователям, необходимый для градиентного бустинга.\n",
        "    test_query: Series\n",
        "      Правило для разбиение тестовой выборки по пользователям, необходимый для градиентного бустинга.\n",
        "    K_items: list\n",
        "      Сколько айтемов учитывать при вычислении метрики качества. По умолчанию равно [20].\n",
        "\n",
        "    На выход:\n",
        "    ----------\n",
        "    model_gbm: LGBMRanker\n",
        "      Обученная ранжирующая модель градиентного бустинга.\n",
        "    \"\"\"\n",
        "    model_gbm = lightgbm.LGBMRanker(n_estimators=parameters[\"n_estimators\"],\n",
        "                                    objective=parameters[\"objective\"],\n",
        "                                    random_state=42)\n",
        "    model_gbm.fit(X_train,\n",
        "                  y_train,\n",
        "                  group=train_query,\n",
        "                  eval_set=[(X_test, y_test)],\n",
        "                  eval_group=[list(test_query)],\n",
        "                  eval_at=K_items)\n",
        "\n",
        "    return model_gbm\n",
        "\n",
        "  def recommend(self, model_gbm, x_test, K_items=20):\n",
        "    \"\"\"\n",
        "    Мотод построит рекомендации по K_items товаров для каждого пользователя.\n",
        "\n",
        "    Параметры:\n",
        "    ----------\n",
        "    model_gbm: LGBMRanker\n",
        "      Обученная ранжирующая модель градиентного бустинга.\n",
        "    x_test: Dataframe\n",
        "      pandas-датафрейм, состоящий из пользователей, для которых необходимо сделать прогноз, и фичей товаров (ранги и скоры), полученные по моделям перовго уровня.\n",
        "    K_items: int\n",
        "      Сколько товаров необходимо порекомендовать каждому пользователю из датафрейма x_test. По умолчанию равно 20.\n",
        "\n",
        "    На выход:\n",
        "    dataset_predicted: dict\n",
        "      Словарь, ключами которого являются user_id пользователей из x_test, а значениями - рекомендованные им товары.\n",
        "    \"\"\"\n",
        "\n",
        "    lgb_test = x_test.copy()\n",
        "    # lgb_test[[\"user_id\", \"item_id\"]].drop_duplicates(inplace=True)\n",
        "    lgb_test.set_index([\"user_id\", \"item_id\"], inplace=True)\n",
        "    lgb_test[\"lgb_score\"] = model_gbm.predict(lgb_test)\n",
        "    # lgb_test = lgb_test.set_index(\"lgb_score\", append=True).sort_values(\"lgb_score\", ascending=False)\n",
        "    # lgb_test.drop_duplicates(inplace=True)\n",
        "\n",
        "    dataset_predicted = dict()\n",
        "    lgb_test.reset_index(inplace=True)\n",
        "    for user, group in tqdm(lgb_test.groupby(\"user_id\")):\n",
        "        dataset_predicted[user] = list(group.sort_values(by=\"lgb_score\", ascending=False).item_id)[:K_items]\n",
        "    return dataset_predicted"
      ],
      "metadata": {
        "id": "gjFlu-h7cVYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение"
      ],
      "metadata": {
        "id": "akbHhFLHIXbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью созданных классов построим решение задачи. Первая реализация использует предпосчитанные скоры. Полный вариант немного ниже."
      ],
      "metadata": {
        "id": "QGht54-AHyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ###\n",
        "# Подготовка данных для обучения моделей отбора кандидатов\n",
        "data_preparator = DataPreparation()\n",
        "\n",
        "df = data_preparator.extract_reluctant_users(data=df,\n",
        "                                             threshold=20,\n",
        "                                             both=False)\n",
        "\n",
        "df = data_preparator.drop_rare_items(data=df,\n",
        "                                      threshold=10)\n",
        "\n",
        "train_global, test_global = data_preparator.train_test(data=df,\n",
        "                                                        by=\"time\",\n",
        "                                                        test_weeks=1)\n",
        "train_global, test_global = data_preparator.common_only(data_first=train_global,\n",
        "                                                        data_second=test_global,\n",
        "                                                        both=False,\n",
        "                                                        common_col=\"user_id\")\n",
        "\n",
        "train_global = data_preparator.extract_reluctant_users(train_global)\n",
        "train_global = data_preparator.drop_rare_items(data=train_global,\n",
        "                                               threshold=20)\n",
        "train_global, test_global = data_preparator.common_only(data_first=train_global,\n",
        "                                                        data_second=test_global,\n",
        "                                                        both=True)\n",
        "\n",
        "train_global_csr, test_global_csr, train, test, users, items = data_preparator.csr_matrix_via_encoder(data_first=train_global,\n",
        "                                                                                                      data_second=test_global)\n",
        "\n",
        "\n",
        "# Скоры вычислены заранее, поэтому заново обучтаь модели не требуется.\n",
        "\n",
        "\n",
        "###\n",
        "# Извлечение кандидатов. Скоры вычислены заранее. Полный вариант представлен разделом ниже.\n",
        "\n",
        "candidates_extractor = CandidatesExtractor()\n",
        "\n",
        "warp_pairs, warp_dict = candidates_extractor.candidates_extraction(model_type=\"warp\",\n",
        "                                                                   path=\"/content/drive/MyDrive/WB School/L4/\",\n",
        "                                                                   prefix=\"global\",\n",
        "                                                                   users=users,\n",
        "                                                                   items=items,\n",
        "                                                                   top=50,\n",
        "                                                                   items_number=50,\n",
        "                                                                   precomputed_scores=True) # 2.94 GB\n",
        "\n",
        "lmf_pairs, lmf_dict = candidates_extractor.candidates_extraction(model_type=\"lmf\",\n",
        "                                                                   path=\"/content/drive/MyDrive/WB School/L4/\",\n",
        "                                                                   prefix=\"global\",\n",
        "                                                                   users=users,\n",
        "                                                                   items=items,\n",
        "                                                                   top=50,\n",
        "                                                                   items_number=50,\n",
        "                                                                   precomputed_scores=True) # 2.52 GB\n",
        "\n",
        "del df, users, items, train_global, test_global\n",
        "\n",
        "\n",
        "###\n",
        "# Бустинг над кандидатами\n",
        "hybrid_recommender = HybridRecommender()\n",
        "dataset = hybrid_recommender.prepare_data(model_first_pairs=warp_pairs,\n",
        "                                          model_second_pairs=lmf_pairs,\n",
        "                                          model_first_dict=warp_dict,\n",
        "                                          model_second_dict=lmf_dict,\n",
        "                                          model_first_name=\"warp\",\n",
        "                                          model_second_name=\"lmf\",\n",
        "                                          test_global_csr=test_global_csr,\n",
        "                                          train=train,\n",
        "                                          test=test) # 0.97 GB\n",
        "del warp_pairs, lmf_pairs, warp_dict, lmf_dict, test_global_csr, train, test # -0.62 GB\n",
        "\n",
        "X_train, y_train, X_test, y_test, train_query, test_query = hybrid_recommender.train_test(data=dataset, train_size=0.7, random_state=42) # 1.09 GB\n",
        "\n",
        "parameters = {\"n_estimators\": 200,\n",
        "              \"objective\": \"lambdarank\"}\n",
        "model_gbm = hybrid_recommender.fit(parameters, X_train, y_train, X_test, y_test, train_query, test_query, K_items=[20])"
      ],
      "metadata": {
        "id": "HENeWR4gIETl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полная реализация алгоритма."
      ],
      "metadata": {
        "id": "yqgTDDBkOYHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ###\n",
        "# # Подготовка данных для обучения моделей отбора кандидатов\n",
        "data_preparator = DataPreparation()\n",
        "\n",
        "df = data_preparator.extract_reluctant_users(data=df,\n",
        "                                             threshold=20,\n",
        "                                             both=False)\n",
        "\n",
        "df = data_preparator.drop_rare_items(data=df,\n",
        "                                      threshold=10)\n",
        "\n",
        "train_global, test_global = data_preparator.train_test(data=df,\n",
        "                                                        by=\"time\",\n",
        "                                                        test_weeks=1)\n",
        "train_global, test_global = data_preparator.common_only(data_first=train_global,\n",
        "                                                        data_second=test_global,\n",
        "                                                        both=False,\n",
        "                                                        common_col=\"user_id\")\n",
        "\n",
        "train_global = data_preparator.extract_reluctant_users(train_global)\n",
        "train_global = data_preparator.drop_rare_items(data=train_global,\n",
        "                                               threshold=20)\n",
        "train_global, test_global = data_preparator.common_only(data_first=train_global,\n",
        "                                                        data_second=test_global,\n",
        "                                                        both=True)\n",
        "\n",
        "train_global_csr, test_global_csr, train, test, users, items = data_preparator.csr_matrix_via_encoder(data_first=train_global,\n",
        "                                                                                                      data_second=test_global)\n",
        "\n",
        "# ###\n",
        "# # Обучение моделей отбора кандидатов\n",
        "train_lightfm = TrainLightFM()\n",
        "\n",
        "parameters_warp = {\"no_components\": 40,\n",
        "                   \"learning_schedule\": \"adagrad\",\n",
        "                   \"learning_rate\": 0.04,\n",
        "                   \"item_alpha\": 0.00001,\n",
        "                   \"user_alpha\": 0.0001,\n",
        "                   \"max_sampled\": 40,\n",
        "                   \"epochs\": 20}\n",
        "model_warp = train_lightfm.fit(train_data_csr=train_global_csr,\n",
        "                               loss=\"warp\",\n",
        "                               parameters=parameters_warp,\n",
        "                               random_state=42)\n",
        "\n",
        "parameters_lmf = {\"no_components\": 11,\n",
        "                  \"learning_schedule\": \"adagrad\",\n",
        "                  \"learning_rate\": 0.019,\n",
        "                   \"item_alpha\": 0.00023,\n",
        "                   \"user_alpha\": 0.00017,\n",
        "                   \"epochs\": 20}\n",
        "model_lmf = train_lightfm.fit(train_data_csr=train_global_csr,\n",
        "                               loss=\"logistic\",\n",
        "                               parameters=parameters_lmf,\n",
        "                               random_state=42)\n",
        "\n",
        "###\n",
        "# Извлечение кандидатов. Скоры вычислены заранее. Полный вариант представлен разделом ниже.\n",
        "\n",
        "candidates_extractor = CandidatesExtractor()\n",
        "\n",
        "warp_pairs, warp_dict = candidates_extractor.candidates_extraction(users=users,\n",
        "                                                                   items=items,\n",
        "                                                                   model=model_warp,\n",
        "                                                                   top=50,\n",
        "                                                                   items_number=50,\n",
        "                                                                   precomputed_scores=False)\n",
        "\n",
        "lmf_pairs, lmf_dict = candidates_extractor.candidates_extractor.candidates_extraction(users=users,\n",
        "                                                                                      items=items,\n",
        "                                                                                      model=model_warp,\n",
        "                                                                                      top=50,\n",
        "                                                                                      items_number=50,\n",
        "                                                                                      precomputed_scores=False)\n",
        "\n",
        "del df, users, items, train_global, test_global\n",
        "\n",
        "###\n",
        "# Бустинг над кандидатами\n",
        "hybrid_recommender = HybridRecommender()\n",
        "dataset = hybrid_recommender.prepare_data(model_first_pairs=warp_pairs,\n",
        "                                          model_second_pairs=lmf_pairs,\n",
        "                                          model_first_dict=warp_dict,\n",
        "                                          model_second_dict=lmf_dict,\n",
        "                                          model_first_name=\"warp\",\n",
        "                                          model_second_name=\"lmf\",\n",
        "                                          test_global_csr=test_global_csr,\n",
        "                                          train=train,\n",
        "                                          test=test) # 0.97 GB\n",
        "del warp_pairs, lmf_pairs, warp_dict, lmf_dict, test_global_csr, train, test # -0.62 GB\n",
        "\n",
        "X_train, y_train, X_test, y_test, train_query, test_query = hybrid_recommender.train_test(data=dataset, train_size=0.7, random_state=42) # 1.09 GB\n",
        "\n",
        "parameters = {\"n_estimators\": 200,\n",
        "              \"objective\": \"lambdarank\"}\n",
        "model_gbm = hybrid_recommender.fit(parameters, X_train, y_train, X_test, y_test, train_query, test_query, K_items=[20])"
      ],
      "metadata": {
        "id": "CWdTpSgOOaQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}